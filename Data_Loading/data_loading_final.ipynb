{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "952a2a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'datawarehouse_aw' already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wx/4qyr26_j0n18b825vfyyz9vm0000gn/T/ipykernel_7576/137155379.py:20: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  result = connection.execute(check_db_query).scalar()\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "engine = create_engine('postgresql://postgres:Karjin545cdac2@172.16.86.130:5432/postgres')\n",
    "\n",
    "# The name of the new database we want to create\n",
    "new_database_name = 'datawarehouse_aw'\n",
    "\n",
    "# Create a connection with isolation level set to AUTOCOMMIT\n",
    "with engine.connect() as connection:\n",
    "    # Use the raw DBAPI connection to set the isolation level to AUTOCOMMIT\n",
    "    raw_connection = connection.connection\n",
    "    raw_connection.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    \n",
    "    # SQL command to check if the database already exists\n",
    "    check_db_query = f\"SELECT 1 FROM pg_database WHERE datname = '{new_database_name}'\"\n",
    "    \n",
    "    # Execute the check query\n",
    "    result = connection.execute(check_db_query).scalar()\n",
    "    \n",
    "    # If the database does not exist, create it\n",
    "    if not result:\n",
    "        create_db_query = f\"CREATE DATABASE {new_database_name}\"\n",
    "        connection.exec_driver_sql(create_db_query)\n",
    "        print(f\"Database '{new_database_name}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Database '{new_database_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "103f8ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL connection details\n",
    "db_user = 'postgres'\n",
    "db_password = 'Karjin545cdac2'\n",
    "db_host = '172.16.86.130'\n",
    "db_port = '5432'\n",
    "db_name = new_database_name\n",
    "github_token = \"Value_for_Gittoken_here\"\n",
    "\n",
    "headers = {'Authorization': f'Bearer {github_token}'}\n",
    "\n",
    "# Create the PostgreSQL engine\n",
    "engine = create_engine(f'postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{new_database_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d82d15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema 'Production' created successfully.\n",
      "Schema 'HumanResources' created successfully.\n",
      "Schema 'Sales' created successfully.\n",
      "Schema 'Purchasing' created successfully.\n",
      "Schema 'Person' created successfully.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# List of schemas to create\n",
    "schemas = [\"Production\", \"HumanResources\", \"Sales\", \"Purchasing\", \"Person\"]\n",
    "\n",
    "# Execute the query to create the schemas\n",
    "with engine.connect() as connection:\n",
    "    for schema in schemas:\n",
    "        create_schema_query = text(f'CREATE SCHEMA IF NOT EXISTS \"{schema}\"')\n",
    "        connection.execute(create_schema_query)\n",
    "        print(f\"Schema '{schema}' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c678651a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and cleaned data from Sales.CountryRegionCurrency.csv into DataFrame.\n",
      "Loaded and cleaned data from Sales.CreditCard.csv into DataFrame.\n",
      "Loaded and cleaned data from Sales.Currency.csv into DataFrame.\n",
      "Loaded and cleaned data from Sales.CurrencyRate.csv into DataFrame.\n",
      "Loaded and cleaned data from Sales.Customer.csv into DataFrame.\n",
      "Loaded and cleaned data from Sales.PersonCreditCard.csv into DataFrame.\n",
      "Loaded and cleaned data from Sales.SalesOrderDetail.csv into DataFrame.\n",
      "Loaded and cleaned data from Sales.SalesOrderHeader.csv into DataFrame.\n",
      "Loaded and cleaned data from Sales.SalesOrderHeaderSalesReason.csv into DataFrame.\n",
      "Loaded and cleaned data from Sales.SalesPerson.csv into DataFrame.\n",
      "Loaded and cleaned data from Sales.SalesPersonQuotaHistory.csv into DataFrame.\n",
      "Loaded and cleaned data from Sales.SalesReason.csv into DataFrame.\n",
      "Loaded and cleaned data from Sales.SalesTaxRate.csv into DataFrame.\n",
      "Loaded and cleaned data from Sales.SalesTerritory.csv into DataFrame.\n",
      "Loaded and cleaned data from Sales.SalesTerritoryHistory.csv into DataFrame.\n",
      "Loaded and cleaned data from Sales.ShoppingCartItem.csv into DataFrame.\n",
      "Loaded and cleaned data from Sales.SpecialOffer.csv into DataFrame.\n",
      "Loaded and cleaned data from Sales.SpecialOfferProduct.csv into DataFrame.\n",
      "Loaded and cleaned data from Sales.Store.csv into DataFrame.\n",
      "DataFrame CountryRegionCurrency uploaded to PostgreSQL successfully.\n",
      "DataFrame CreditCard uploaded to PostgreSQL successfully.\n",
      "DataFrame Currency uploaded to PostgreSQL successfully.\n",
      "DataFrame CurrencyRate uploaded to PostgreSQL successfully.\n",
      "DataFrame Customer uploaded to PostgreSQL successfully.\n",
      "DataFrame PersonCreditCard uploaded to PostgreSQL successfully.\n",
      "DataFrame SalesOrderDetail uploaded to PostgreSQL successfully.\n",
      "DataFrame SalesOrderHeader uploaded to PostgreSQL successfully.\n",
      "DataFrame SalesOrderHeaderSalesReason uploaded to PostgreSQL successfully.\n",
      "DataFrame SalesPerson uploaded to PostgreSQL successfully.\n",
      "DataFrame SalesPersonQuotaHistory uploaded to PostgreSQL successfully.\n",
      "DataFrame SalesReason uploaded to PostgreSQL successfully.\n",
      "DataFrame SalesTaxRate uploaded to PostgreSQL successfully.\n",
      "DataFrame SalesTerritory uploaded to PostgreSQL successfully.\n",
      "DataFrame SalesTerritoryHistory uploaded to PostgreSQL successfully.\n",
      "DataFrame ShoppingCartItem uploaded to PostgreSQL successfully.\n",
      "DataFrame SpecialOffer uploaded to PostgreSQL successfully.\n",
      "DataFrame SpecialOfferProduct uploaded to PostgreSQL successfully.\n",
      "DataFrame Store uploaded to PostgreSQL successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# GitHub repository details\n",
    "owner = \"kartikcdac\"\n",
    "repo = \"Project_CDAC\"\n",
    "branch = \"main\"  # e.g., 'main' or 'master'\n",
    "path = \"Database_files/Sales\"  # Path within the repo\n",
    "\n",
    "# GitHub API URL to list files in the repository\n",
    "sales_api_url = f\"https://api.github.com/repos/kartikcdac/Project_CDAC/contents/Database_files/Sales?ref=main\"\n",
    "\n",
    "# Make a GET request to the GitHub API\n",
    "response = requests.get(sales_api_url,headers=headers)\n",
    "files = response.json()\n",
    "\n",
    "# Check if response is valid\n",
    "if not isinstance(files, list):\n",
    "    raise ValueError(\"Error fetching files. Response is not a list.\")\n",
    "\n",
    "# Initialize an empty dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "\n",
    "primary_keys = {\n",
    "    \"Sales.SalesOrderDetail\": [\"SalesOrderID\", \"SalesOrderDetailID\"],\n",
    "    \"Sales.SalesOrderHeader\": [\"SalesOrderID\"],\n",
    "    \"Sales.SalesOrderHeaderSalesReason\": [\"SalesOrderID\", \"SalesReasonID\"],\n",
    "    \"Sales.SalesPersonQuotaHistory\": [\"BusinessEntityID\", \"QuotaDate\"],\n",
    "    \"Sales.SalesReason\": [\"SalesReasonID\"],\n",
    "    \"Sales.SalesTaxRate\": [\"SalesTaxRateID\"],\n",
    "    \"Sales.SalesTerritory\": [\"TerritoryID\"],\n",
    "    \"Sales.ShoppingCartItem\": [\"ShoppingCartItemID\"],\n",
    "    \"Sales.SpecialOffer\": [\"SpecialOfferID\"],\n",
    "    \"Sales.SpecialOfferProduct\": [\"SpecialOfferID\", \"ProductID\"],\n",
    "    \"Sales.Store\": [\"BusinessEntityID\"],\n",
    "    \"Sales.CountryRegionCurrency\": [\"CountryRegionCode\", \"CurrencyCode\"],\n",
    "    \"Sales.CreditCard\": [\"CreditCardID\"],\n",
    "    \"Sales.Currency\": [\"CurrencyCode\"],\n",
    "    \"Sales.CurrencyRate\": [\"CurrencyRateID\"],\n",
    "    \"Sales.Customer\": [\"CustomerID\"],\n",
    "    \"Sales.PersonCreditCard\": [\"BusinessEntityID\", \"CreditCardID\"],\n",
    "    \"Sales.SalesPerson\": [\"BusinessEntityID\"],\n",
    "    \"Sales.SalesTerritoryHistory\": [\"BusinessEntityID\", \"StartDate\", \"TerritoryID\"]\n",
    "    }\n",
    "\n",
    "# Loop through each file in the repository\n",
    "for file in files:\n",
    "    if file['name'].endswith('.csv'):\n",
    "        # Get the raw URL of the CSV file\n",
    "        csv_url = file['download_url']\n",
    "        \n",
    "        # Download the CSV file\n",
    "        csv_response = requests.get(csv_url)\n",
    "        \n",
    "        # Check if response is valid\n",
    "        if csv_response.status_code == 200:\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(StringIO(csv_response.text))\n",
    "            \n",
    "            # Store the DataFrame in the dictionary with the filename as the key\n",
    "            file_name = file['name'].replace('.csv', '')\n",
    "            schema_name = file_name.split('.')[0]\n",
    "            table_name = file_name.split('.')[1]\n",
    "            \n",
    "            # Check for duplicates in primary key columns and remove them\n",
    "            primary_key_columns = primary_keys.get(f\"{schema_name}.{table_name}\", [])\n",
    "            if primary_key_columns:\n",
    "                # Remove rows where any primary key column contains null values\n",
    "                df.dropna(subset=primary_key_columns, inplace=True)\n",
    "                \n",
    "                # Remove duplicate rows based on the primary key columns\n",
    "                df.drop_duplicates(subset=primary_key_columns, inplace=True)\n",
    "                \n",
    "            # Clean the DataFrame by dropping rows with missing values\n",
    "            df.dropna(how='all',inplace=True)\n",
    "            \n",
    "            dataframes[table_name] = df\n",
    "            print(f\"Loaded and cleaned data from {file['name']} into DataFrame.\")\n",
    "        else:\n",
    "            print(f\"Failed to download {file['name']}.\")\n",
    "\n",
    "# Validation function\n",
    "def validate_dataframe(df, file_name):\n",
    "    # Example validations (customize these as per your schema requirements)\n",
    "    \n",
    "    # Check data types (automatically inferred types)\n",
    "    inferred_dtypes = df.dtypes\n",
    "    print(f\"Data types for {table_name}:\\n{inferred_dtypes}\")\n",
    "\n",
    "    # Comprehensive data type expectations for validation\n",
    "    expected_type_map = {\n",
    "        'int64': 'int',\n",
    "        'float64': 'float',\n",
    "        'object': 'string',  # General string type\n",
    "        'bool': 'bool',\n",
    "        'datetime64[ns]': 'datetime',\n",
    "        'timedelta[ns]': 'timespan',\n",
    "        'category': 'categorical',\n",
    "        # PostgreSQL-specific mappings\n",
    "        'varchar': 'string',\n",
    "        'nvarchar': 'string',\n",
    "        'timestamp': 'datetime',\n",
    "        'text': 'string'\n",
    "    }\n",
    "\n",
    "    for column, dtype in inferred_dtypes.items():\n",
    "        expected_type = expected_type_map.get(str(dtype), None)\n",
    "        if expected_type is None:\n",
    "            print(f\"Warning: {table_name} column {column} has an unexpected data type: {dtype}\")\n",
    "\n",
    "\n",
    "# Continue with the upload\n",
    "for table_name, df in dataframes.items():\n",
    "    try:\n",
    "        df.to_sql(table_name, engine, schema=schema_name, if_exists='replace', index=False)\n",
    "        print(f\"DataFrame {table_name} uploaded to PostgreSQL successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading {table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4debe720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and cleaned data from Production.BillOfMaterials.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.Culture.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.Document.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.Location.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.Product.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.ProductCategory.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.ProductCostHistory.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.ProductDescription.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.ProductDocument.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.ProductInventory.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.ProductListPriceHistory.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.ProductModel.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.ProductModelIllustration.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.ProductModelProductDescriptionCulture.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.ProductPhoto.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.ProductReview.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.ProductSubCategory.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.ScrapReason.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.TransactionHistory.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.TranscationHistoryArchive.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.UnitMeasure.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.WorkOrder.csv into DataFrame.\n",
      "Loaded and cleaned data from Production.WorkOrderRouting.csv into DataFrame.\n",
      "DataFrame BillOfMaterials uploaded to PostgreSQL successfully.\n",
      "DataFrame Culture uploaded to PostgreSQL successfully.\n",
      "DataFrame Document uploaded to PostgreSQL successfully.\n",
      "DataFrame Location uploaded to PostgreSQL successfully.\n",
      "DataFrame Product uploaded to PostgreSQL successfully.\n",
      "DataFrame ProductCategory uploaded to PostgreSQL successfully.\n",
      "DataFrame ProductCostHistory uploaded to PostgreSQL successfully.\n",
      "DataFrame ProductDescription uploaded to PostgreSQL successfully.\n",
      "DataFrame ProductDocument uploaded to PostgreSQL successfully.\n",
      "DataFrame ProductInventory uploaded to PostgreSQL successfully.\n",
      "DataFrame ProductListPriceHistory uploaded to PostgreSQL successfully.\n",
      "DataFrame ProductModel uploaded to PostgreSQL successfully.\n",
      "DataFrame ProductModelIllustration uploaded to PostgreSQL successfully.\n",
      "DataFrame ProductModelProductDescriptionCulture uploaded to PostgreSQL successfully.\n",
      "DataFrame ProductPhoto uploaded to PostgreSQL successfully.\n",
      "DataFrame ProductReview uploaded to PostgreSQL successfully.\n",
      "DataFrame ProductSubCategory uploaded to PostgreSQL successfully.\n",
      "DataFrame ScrapReason uploaded to PostgreSQL successfully.\n",
      "DataFrame TransactionHistory uploaded to PostgreSQL successfully.\n",
      "DataFrame TranscationHistoryArchive uploaded to PostgreSQL successfully.\n",
      "DataFrame UnitMeasure uploaded to PostgreSQL successfully.\n",
      "DataFrame WorkOrder uploaded to PostgreSQL successfully.\n",
      "DataFrame WorkOrderRouting uploaded to PostgreSQL successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# GitHub repository details\n",
    "owner = \"kartikcdac\"\n",
    "repo = \"Project_CDAC\"\n",
    "branch = \"main\"  # e.g., 'main' or 'master'\n",
    "path = \"Database_files/Production\"  # Path within the repo\n",
    "\n",
    "# GitHub API URL to list files in the repository\n",
    "production_api_url = f\"https://api.github.com/repos/kartikcdac/Project_CDAC/contents/Database_files/Production?ref=main\"\n",
    "\n",
    "# Make a GET request to the GitHub API\n",
    "response = requests.get(production_api_url,headers=headers)\n",
    "files = response.json()\n",
    "\n",
    "# Check if response is valid\n",
    "if not isinstance(files, list):\n",
    "    raise ValueError(\"Error fetching files. Response is not a list.\")\n",
    "\n",
    "# Initialize an empty dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "primary_keys = {\n",
    "    \"Production.Location\": [\"LocationID\"], \n",
    "    \"Production.ScrapReason\": [\"ScrapReasonID\"],\n",
    "    \"Production.TransactionHistory\": [\"TransactionID\"],\n",
    "    \"Production.TransactionHistoryArchive\": [\"TransactionID\"],\n",
    "    \"Production.UnitMeasure\": [\"UnitMeasureCode\"],\n",
    "    \"Production.WorkOrder\": [\"WorkOrderID\"],\n",
    "    \"Production.WorkOrderRouting\": [\"WorkOrderID\", \"ProductID\", \"OperationSequence\"],\n",
    "    \"Production.BillOfMaterials\": [\"BillOfMaterialsID\"],\n",
    "    \"Production.Culture\": [\"CultureID\"],\n",
    "    \"Production.Illustration\": [\"IllustrationID\"],\n",
    "    \"Production.Product\": [\"ProductID\"],\n",
    "    \"Production.ProductCategory\": [\"ProductCategoryID\"],\n",
    "    \"Production.ProductCostHistory\": [\"ProductID\", \"StartDate\"],\n",
    "    \"Production.ProductDescription\": [\"ProductDescriptionID\"],\n",
    "    \"Production.ProductInventory\": [\"ProductID\", \"LocationID\"],\n",
    "    \"Production.ProductListPriceHistory\": [\"ProductID\", \"StartDate\"],\n",
    "    \"Production.ProductModel\": [\"ProductModelID\"],\n",
    "    \"Production.ProductReview\": [\"ProductReviewID\"],\n",
    "    \"Production.ProductSubCategory\": [\"ProductSubcategoryID\"]\n",
    "    }\n",
    "\n",
    "# Loop through each file in the repository\n",
    "for file in files:\n",
    "    if file['name'].endswith('.csv'):\n",
    "        # Get the raw URL of the CSV file\n",
    "        csv_url = file['download_url']\n",
    "        \n",
    "        # Download the CSV file\n",
    "        csv_response = requests.get(csv_url)\n",
    "        \n",
    "        # Check if response is valid\n",
    "        if csv_response.status_code == 200:\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(StringIO(csv_response.text))\n",
    "            \n",
    "            # Store the DataFrame in the dictionary with the filename as the key\n",
    "            file_name = file['name'].replace('.csv', '')\n",
    "            schema_name = file_name.split('.')[0]\n",
    "            table_name = file_name.split('.')[1]\n",
    "            \n",
    "            # Check for duplicates in primary key columns and remove them\n",
    "            primary_key_columns = primary_keys.get(f\"{schema_name}.{table_name}\", [])\n",
    "            if primary_key_columns:\n",
    "                # Remove rows where any primary key column contains null values\n",
    "                df.dropna(subset=primary_key_columns, inplace=True)\n",
    "                \n",
    "                # Remove duplicate rows based on the primary key columns\n",
    "                df.drop_duplicates(subset=primary_key_columns, inplace=True)\n",
    "                \n",
    "            # Clean the DataFrame by dropping rows with missing values\n",
    "            df.dropna(how='all',inplace=True)\n",
    "            \n",
    "            dataframes[table_name] = df\n",
    "            print(f\"Loaded and cleaned data from {file['name']} into DataFrame.\")\n",
    "        else:\n",
    "            print(f\"Failed to download {file['name']}.\")\n",
    "\n",
    "\n",
    "# Validation function\n",
    "def validate_dataframe(df, file_name):\n",
    "    # Example validations (customize these as per your schema requirements)\n",
    "    \n",
    "    # Check data types (automatically inferred types)\n",
    "    inferred_dtypes = df.dtypes\n",
    "    print(f\"Data types for {table_name}:\\n{inferred_dtypes}\")\n",
    "\n",
    "    # Comprehensive data type expectations for validation\n",
    "    expected_type_map = {\n",
    "        'int64': 'int',\n",
    "        'float64': 'float',\n",
    "        'object': 'string',  # General string type\n",
    "        'bool': 'bool',\n",
    "        'datetime64[ns]': 'datetime',\n",
    "        'timedelta[ns]': 'timespan',\n",
    "        'category': 'categorical',\n",
    "        # PostgreSQL-specific mappings\n",
    "        'varchar': 'string',\n",
    "        'nvarchar': 'string',\n",
    "        'timestamp': 'datetime',\n",
    "        'text': 'string'\n",
    "    }\n",
    "\n",
    "    for column, dtype in inferred_dtypes.items():\n",
    "        expected_type = expected_type_map.get(str(dtype), None)\n",
    "        if expected_type is None:\n",
    "            print(f\"Warning: {table_name} column {column} has an unexpected data type: {dtype}\")\n",
    "    \n",
    "\n",
    "# Continue with the upload\n",
    "for table_name, df in dataframes.items():\n",
    "    try:\n",
    "        df.to_sql(table_name, engine, schema=schema_name, if_exists='replace', index=False)\n",
    "        print(f\"DataFrame {table_name} uploaded to PostgreSQL successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading {table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25e5457a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and cleaned data from HumanResources.Department.csv into DataFrame.\n",
      "Loaded and cleaned data from HumanResources.Employee.csv into DataFrame.\n",
      "Loaded and cleaned data from HumanResources.EmployeeDepartmentHistory.csv into DataFrame.\n",
      "Loaded and cleaned data from HumanResources.EmployeePayHistory.csv into DataFrame.\n",
      "Loaded and cleaned data from HumanResources.JobCandidate.csv into DataFrame.\n",
      "Loaded and cleaned data from HumanResources.Shift.csv into DataFrame.\n",
      "DataFrame Department uploaded to PostgreSQL successfully.\n",
      "DataFrame Employee uploaded to PostgreSQL successfully.\n",
      "DataFrame EmployeeDepartmentHistory uploaded to PostgreSQL successfully.\n",
      "DataFrame EmployeePayHistory uploaded to PostgreSQL successfully.\n",
      "DataFrame JobCandidate uploaded to PostgreSQL successfully.\n",
      "DataFrame Shift uploaded to PostgreSQL successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# GitHub repository details\n",
    "owner = \"kartikcdac\"\n",
    "repo = \"Project_CDAC\"\n",
    "branch = \"main\"  # e.g., 'main' or 'master'\n",
    "path = \"Database_files/HumanResources\"  # Path within the repo\n",
    "\n",
    "# GitHub API URL to list files in the repository\n",
    "hr_api_url = f\"https://api.github.com/repos/kartikcdac/Project_CDAC/contents/Database_files/HumanResources?ref=main\"\n",
    "\n",
    "# Make a GET request to the GitHub API\n",
    "response = requests.get(hr_api_url,headers=headers)\n",
    "files = response.json()\n",
    "\n",
    "# Check if response is valid\n",
    "if not isinstance(files, list):\n",
    "    raise ValueError(\"Error fetching files. Response is not a list.\")\n",
    "\n",
    "# Initialize an empty dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "primary_keys = {\n",
    "    \"HumanResources.Shift\": [\"ShiftID\"],\n",
    "    \"HumanResources.Department\": [\"DepartmentID\"],\n",
    "    \"HumanResources.Employee\": [\"BusinessEntityID\"],\n",
    "    \"HumanResources.EmployeeDepartmentHistory\": [\"BusinessEntityID\", \"StartDate\", \"DepartmentID\", \"ShiftID\"],\n",
    "    \"HumanResources.EmployeePayHistory\": [\"BusinessEntityID\", \"RateChangeDate\"],\n",
    "    \"HumanResources.JobCandidate\": [\"JobCandidateID\"]\n",
    "}\n",
    "\n",
    "# Loop through each file in the repository\n",
    "for file in files:\n",
    "    if file['name'].endswith('.csv'):\n",
    "        # Get the raw URL of the CSV file\n",
    "        csv_url = file['download_url']\n",
    "        \n",
    "        # Download the CSV file\n",
    "        csv_response = requests.get(csv_url)\n",
    "        \n",
    "        # Check if response is valid\n",
    "        if csv_response.status_code == 200:\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(StringIO(csv_response.text))\n",
    "            \n",
    "            # Store the DataFrame in the dictionary with the filename as the key\n",
    "            file_name = file['name'].replace('.csv', '')\n",
    "            schema_name = file_name.split('.')[0]\n",
    "            table_name = file_name.split('.')[1]\n",
    "            \n",
    "            # Check for duplicates in primary key columns and remove them\n",
    "            primary_key_columns = primary_keys.get(f\"{schema_name}.{table_name}\", [])\n",
    "            if primary_key_columns:\n",
    "                # Remove rows where any primary key column contains null values\n",
    "                df.dropna(subset=primary_key_columns, inplace=True)\n",
    "                \n",
    "                # Remove duplicate rows based on the primary key columns\n",
    "                df.drop_duplicates(subset=primary_key_columns, inplace=True)\n",
    "                \n",
    "            # Clean the DataFrame by dropping rows with missing values\n",
    "            df.dropna(how='all',inplace=True)\n",
    "            \n",
    "            dataframes[table_name] = df\n",
    "            print(f\"Loaded and cleaned data from {file['name']} into DataFrame.\")\n",
    "        else:\n",
    "            print(f\"Failed to download {file['name']}.\")\n",
    "\n",
    "\n",
    "# Validation function\n",
    "def validate_dataframe(df, file_name):\n",
    "    # Example validations (customize these as per your schema requirements)\n",
    "    \n",
    "    # Check data types (automatically inferred types)\n",
    "    inferred_dtypes = df.dtypes\n",
    "    print(f\"Data types for {table_name}:\\n{inferred_dtypes}\")\n",
    "\n",
    "    # Comprehensive data type expectations for validation\n",
    "    expected_type_map = {\n",
    "        'int64': 'int',\n",
    "        'float64': 'float',\n",
    "        'object': 'string',  # General string type\n",
    "        'bool': 'bool',\n",
    "        'datetime64[ns]': 'datetime',\n",
    "        'timedelta[ns]': 'timespan',\n",
    "        'category': 'categorical',\n",
    "        # PostgreSQL-specific mappings\n",
    "        'varchar': 'string',\n",
    "        'nvarchar': 'string',\n",
    "        'timestamp': 'datetime',\n",
    "        'text': 'string'\n",
    "    }\n",
    "\n",
    "    for column, dtype in inferred_dtypes.items():\n",
    "        expected_type = expected_type_map.get(str(dtype), None)\n",
    "        if expected_type is None:\n",
    "            print(f\"Warning: {table_name} column {column} has an unexpected data type: {dtype}\")\n",
    "\n",
    "\n",
    "# Continue with the upload\n",
    "for table_name, df in dataframes.items():\n",
    "    try:\n",
    "        df.to_sql(table_name, engine, schema=schema_name, if_exists='replace', index=False)\n",
    "        print(f\"DataFrame {table_name} uploaded to PostgreSQL successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading {table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a334ebaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and cleaned data from Purchasing.ProductVendor.csv into DataFrame.\n",
      "Loaded and cleaned data from Purchasing.PurchaseOrderDetail.csv into DataFrame.\n",
      "Loaded and cleaned data from Purchasing.PurchaseOrderHeader.csv into DataFrame.\n",
      "Loaded and cleaned data from Purchasing.ShipMethod.csv into DataFrame.\n",
      "Loaded and cleaned data from Purchasing.Vendor.csv into DataFrame.\n",
      "DataFrame ProductVendor uploaded to PostgreSQL successfully.\n",
      "DataFrame PurchaseOrderDetail uploaded to PostgreSQL successfully.\n",
      "DataFrame PurchaseOrderHeader uploaded to PostgreSQL successfully.\n",
      "DataFrame ShipMethod uploaded to PostgreSQL successfully.\n",
      "DataFrame Vendor uploaded to PostgreSQL successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# GitHub repository details\n",
    "owner = \"kartikcdac\"\n",
    "repo = \"Project_CDAC\"\n",
    "branch = \"main\"  # e.g., 'main' or 'master'\n",
    "path = \"Database_files/Purchasing\"  # Path within the repo\n",
    "\n",
    "# GitHub API URL to list files in the repository\n",
    "purchasing_api_url = f\"https://api.github.com/repos/kartikcdac/Project_CDAC/contents/Database_files/Purchasing?ref=main\"\n",
    "\n",
    "# Make a GET request to the GitHub API\n",
    "response = requests.get(purchasing_api_url,headers=headers)\n",
    "files = response.json()\n",
    "\n",
    "# Check if response is valid\n",
    "if not isinstance(files, list):\n",
    "    raise ValueError(\"Error fetching files. Response is not a list.\")\n",
    "\n",
    "# Initialize an empty dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "primary_keys = {\n",
    "    \"Purchasing.ProductVendor\": [\"ProductID\", \"BusinessEntityID\"],\n",
    "    \"Purchasing.PurchaseOrderDetail\": [\"PurchaseOrderID\", \"PurchaseOrderDetailID\"],\n",
    "    \"Purchasing.PurchaseOrderHeader\": [\"PurchaseOrderID\"],\n",
    "    \"Purchasing.ShipMethod\": [\"ShipMethodID\"],\n",
    "    \"Purchasing.Vendor\": [\"BusinessEntityID\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# Loop through each file in the repository\n",
    "for file in files:\n",
    "    if file['name'].endswith('.csv'):\n",
    "        # Get the raw URL of the CSV file\n",
    "        csv_url = file['download_url']\n",
    "        \n",
    "        # Download the CSV file\n",
    "        csv_response = requests.get(csv_url)\n",
    "        \n",
    "        # Check if response is valid\n",
    "        if csv_response.status_code == 200:\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(StringIO(csv_response.text))\n",
    "            \n",
    "            # Store the DataFrame in the dictionary with the filename as the key\n",
    "            file_name = file['name'].replace('.csv', '')\n",
    "            schema_name = file_name.split('.')[0]\n",
    "            table_name = file_name.split('.')[1]\n",
    "            \n",
    "            # Check for duplicates in primary key columns and remove them\n",
    "            primary_key_columns = primary_keys.get(f\"{schema_name}.{table_name}\", [])\n",
    "            if primary_key_columns:\n",
    "                # Remove rows where any primary key column contains null values\n",
    "                df.dropna(subset=primary_key_columns, inplace=True)\n",
    "                \n",
    "                # Remove duplicate rows based on the primary key columns\n",
    "                df.drop_duplicates(subset=primary_key_columns, inplace=True)\n",
    "                \n",
    "            # Clean the DataFrame by dropping rows with missing values\n",
    "            df.dropna(how='all',inplace=True)\n",
    "            \n",
    "            dataframes[table_name] = df\n",
    "            print(f\"Loaded and cleaned data from {file['name']} into DataFrame.\")\n",
    "        else:\n",
    "            print(f\"Failed to download {file['name']}.\")\n",
    "\n",
    "\n",
    "# Validation function\n",
    "def validate_dataframe(df, file_name):\n",
    "    # Example validations (customize these as per your schema requirements)\n",
    "    \n",
    "    # Check data types (automatically inferred types)\n",
    "    inferred_dtypes = df.dtypes\n",
    "    print(f\"Data types for {table_name}:\\n{inferred_dtypes}\")\n",
    "\n",
    "    # Comprehensive data type expectations for validation\n",
    "    expected_type_map = {\n",
    "        'int64': 'int',\n",
    "        'float64': 'float',\n",
    "        'object': 'string',  # General string type\n",
    "        'bool': 'bool',\n",
    "        'datetime64[ns]': 'datetime',\n",
    "        'timedelta[ns]': 'timespan',\n",
    "        'category': 'categorical',\n",
    "        # PostgreSQL-specific mappings\n",
    "        'varchar': 'string',\n",
    "        'nvarchar': 'string',\n",
    "        'timestamp': 'datetime',\n",
    "        'text': 'string'\n",
    "    }\n",
    "\n",
    "    for column, dtype in inferred_dtypes.items():\n",
    "        expected_type = expected_type_map.get(str(dtype), None)\n",
    "        if expected_type is None:\n",
    "            print(f\"Warning: {table_name} column {column} has an unexpected data type: {dtype}\")\n",
    "    \n",
    "\n",
    "# Continue with the upload\n",
    "for table_name, df in dataframes.items():\n",
    "    try:\n",
    "        df.to_sql(table_name, engine, schema=schema_name, if_exists='replace', index=False)\n",
    "        print(f\"DataFrame {table_name} uploaded to PostgreSQL successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading {table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02b3238c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and cleaned data from Person.Address.csv into DataFrame.\n",
      "Loaded and cleaned data from Person.AddressType.csv into DataFrame.\n",
      "Loaded and cleaned data from Person.BusinessEntity.csv into DataFrame.\n",
      "Loaded and cleaned data from Person.BusinessEntityAddress.csv into DataFrame.\n",
      "Loaded and cleaned data from Person.BusinessEntityContact.csv into DataFrame.\n",
      "Loaded and cleaned data from Person.ContactType.csv into DataFrame.\n",
      "Loaded and cleaned data from Person.CountryRegion.csv into DataFrame.\n",
      "Loaded and cleaned data from Person.EmailAddress.csv into DataFrame.\n",
      "Loaded and cleaned data from Person.Password.csv into DataFrame.\n",
      "Loaded and cleaned data from Person.Person.csv into DataFrame.\n",
      "Loaded and cleaned data from Person.PersonPhone.csv into DataFrame.\n",
      "Loaded and cleaned data from Person.PhoneNumberType.csv into DataFrame.\n",
      "Loaded and cleaned data from Person.StateProvince.csv into DataFrame.\n",
      "DataFrame Address uploaded to PostgreSQL successfully.\n",
      "DataFrame AddressType uploaded to PostgreSQL successfully.\n",
      "DataFrame BusinessEntity uploaded to PostgreSQL successfully.\n",
      "DataFrame BusinessEntityAddress uploaded to PostgreSQL successfully.\n",
      "DataFrame BusinessEntityContact uploaded to PostgreSQL successfully.\n",
      "DataFrame ContactType uploaded to PostgreSQL successfully.\n",
      "DataFrame CountryRegion uploaded to PostgreSQL successfully.\n",
      "DataFrame EmailAddress uploaded to PostgreSQL successfully.\n",
      "DataFrame Password uploaded to PostgreSQL successfully.\n",
      "DataFrame Person uploaded to PostgreSQL successfully.\n",
      "DataFrame PersonPhone uploaded to PostgreSQL successfully.\n",
      "DataFrame PhoneNumberType uploaded to PostgreSQL successfully.\n",
      "DataFrame StateProvince uploaded to PostgreSQL successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# GitHub repository details\n",
    "owner = \"kartikcdac\"\n",
    "repo = \"Project_CDAC\"\n",
    "branch = \"main\"  # e.g., 'main' or 'master'\n",
    "path = \"Database_files/Person\"  # Path within the repo\n",
    "\n",
    "# GitHub API URL to list files in the repository\n",
    "person_api_url = f\"https://api.github.com/repos/kartikcdac/Project_CDAC/contents/Database_files/Person?ref=main\"\n",
    "\n",
    "# Make a GET request to the GitHub API\n",
    "response = requests.get(person_api_url,headers=headers)\n",
    "files = response.json()\n",
    "\n",
    "# Check if response is valid\n",
    "if not isinstance(files, list):\n",
    "    raise ValueError(\"Error fetching files. Response is not a list.\")\n",
    "\n",
    "# Initialize an empty dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "primary_keys = {\n",
    "    \"Person.StateProvince\": [\"StateProvinceID\"],\n",
    "    \"Person.Address\": [\"AddressID\"],\n",
    "    \"Person.AddressType\": [\"AddressTypeID\"],\n",
    "    \"Person.BusinessEntity\": [\"BusinessEntityID\"],\n",
    "    \"Person.BusinessEntityAddress\": [\"BusinessEntityID\", \"AddressID\", \"AddressTypeID\"],\n",
    "    \"Person.BusinessEntityContact\": [\"BusinessEntityID\", \"PersonID\", \"ContactTypeID\"],\n",
    "    \"Person.ContactType\": [\"ContactTypeID\"],\n",
    "    \"Person.CountryRegion\": [\"CountryRegionCode\"],\n",
    "    \"Person.EmailAddress\": [\"BusinessEntityID\", \"EmailAddressID\"],\n",
    "    \"Person.Password\": [\"BusinessEntityID\"],\n",
    "    \"Person.Person\": [\"BusinessEntityID\"],\n",
    "    \"Person.PersonPhone\": [\"BusinessEntityID\", \"PhoneNumber\", \"PhoneNumberTypeID\"],\n",
    "    \"Person.PhoneNumberType\": [\"PhoneNumberTypeID\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# Loop through each file in the repository\n",
    "for file in files:\n",
    "    if file['name'].endswith('.csv'):\n",
    "        # Get the raw URL of the CSV file\n",
    "        csv_url = file['download_url']\n",
    "        \n",
    "        # Download the CSV file\n",
    "        csv_response = requests.get(csv_url)\n",
    "        \n",
    "        # Check if response is valid\n",
    "        if csv_response.status_code == 200:\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(StringIO(csv_response.text))\n",
    "            \n",
    "            # Store the DataFrame in the dictionary with the filename as the key\n",
    "            file_name = file['name'].replace('.csv', '')\n",
    "            schema_name = file_name.split('.')[0]\n",
    "            table_name = file_name.split('.')[1]\n",
    "            \n",
    "            # Check for duplicates in primary key columns and remove them\n",
    "            primary_key_columns = primary_keys.get(f\"{schema_name}.{table_name}\", [])\n",
    "            if primary_key_columns:\n",
    "                # Remove rows where any primary key column contains null values\n",
    "                df.dropna(subset=primary_key_columns, inplace=True)\n",
    "                \n",
    "                # Remove duplicate rows based on the primary key columns\n",
    "                df.drop_duplicates(subset=primary_key_columns, inplace=True)\n",
    "                \n",
    "            # Clean the DataFrame by dropping rows with missing values\n",
    "            df.dropna(how='all',inplace=True)\n",
    "            \n",
    "            dataframes[table_name] = df\n",
    "            print(f\"Loaded and cleaned data from {file['name']} into DataFrame.\")\n",
    "        else:\n",
    "            print(f\"Failed to download {file['name']}.\")\n",
    "\n",
    "\n",
    "# Validation function\n",
    "def validate_dataframe(df, file_name):\n",
    "    # Example validations (customize these as per your schema requirements)\n",
    "    \n",
    "    # Check data types (automatically inferred types)\n",
    "    inferred_dtypes = df.dtypes\n",
    "    print(f\"Data types for {table_name}:\\n{inferred_dtypes}\")\n",
    "\n",
    "    # Comprehensive data type expectations for validation\n",
    "    expected_type_map = {\n",
    "        'int64': 'int',\n",
    "        'float64': 'float',\n",
    "        'object': 'string',  # General string type\n",
    "        'bool': 'bool',\n",
    "        'datetime64[ns]': 'datetime',\n",
    "        'timedelta[ns]': 'timespan',\n",
    "        'category': 'categorical',\n",
    "        # PostgreSQL-specific mappings\n",
    "        'varchar': 'string',\n",
    "        'nvarchar': 'string',\n",
    "        'timestamp': 'datetime',\n",
    "        'text': 'string'\n",
    "    }\n",
    "\n",
    "    for column, dtype in inferred_dtypes.items():\n",
    "        expected_type = expected_type_map.get(str(dtype), None)\n",
    "        if expected_type is None:\n",
    "            print(f\"Warning: {table_name} column {column} has an unexpected data type: {dtype}\")\n",
    "\n",
    "# Continue with the upload\n",
    "for table_name, df in dataframes.items():\n",
    "    try:\n",
    "        df.to_sql(table_name, engine, schema=schema_name, if_exists='replace', index=False)\n",
    "        print(f\"DataFrame {table_name} uploaded to PostgreSQL successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading {table_name}: {e}\")\n",
    "\n",
    "# Optionally, you can close the engine connection after operations are done\n",
    "engine.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
